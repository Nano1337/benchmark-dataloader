DATASET STREAMING BENCHMARKS
Started at: 2025-04-25 07:01:07
Configuration: batch_size=512, num_workers=16, prefetch_factor=4



========== WebDataset ==========
Seed set to 42
/home/ec2-user/benchmark-dataloader/.venv/lib/python3.13/site-packages/webdataset/compat.py:389: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number
  warnings.warn(
Seed set to 42
Using remote path: s3://datology-assets-dev/users/haoli/shards/webdataset
Using remote path: s3://datology-assets-dev/users/haoli/shards/webdataset
Listing objects...
Found 23 objects in S3
Found 23 train tar files matching pattern 'benchmark-train-'
Cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/webdataset_benchmark
Using cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/webdataset_benchmark
Using 16 worker threads for data loading
Starting benchmark with batch size 512

Epoch 1/2: 0it [00:00, ?it/s]
Epoch 1/2: 1it [00:04,  4.99s/it]
Epoch 1/2: 17it [00:07,  2.33it/s]
Epoch 1/2: 33it [00:09,  3.35it/s]
Epoch 1/2: 49it [00:12,  4.08it/s]
Epoch 1/2: 65it [00:14,  4.58it/s]
Epoch 1/2: 81it [00:16,  4.91it/s]
Epoch 1/2: 97it [00:18,  5.18it/s]
Epoch 1/2: 113it [00:22,  5.07it/s]
Epoch 1/2: 129it [00:23,  5.46it/s]
Epoch 1/2: 145it [00:27,  5.34it/s]
Epoch 1/2: 161it [00:30,  5.36it/s]
Epoch 1/2: 177it [00:32,  5.39it/s]
Epoch 1/2: 181it [00:33,  5.48it/s]
Time to first batch: 4.9926s
Epoch 1: Processed 88514 samples in 33.04s (2679.02 images/sec)

Epoch 2/2: 0it [00:00, ?it/s]
Epoch 2/2: 1it [00:02,  2.63s/it]
Epoch 2/2: 17it [00:04,  3.54it/s]
Epoch 2/2: 33it [00:07,  4.49it/s]
Epoch 2/2: 49it [00:09,  5.09it/s]
Epoch 2/2: 65it [00:11,  5.48it/s]
Epoch 2/2: 81it [00:14,  5.69it/s]
Epoch 2/2: 97it [00:16,  5.79it/s]
Epoch 2/2: 113it [00:18,  5.99it/s]
Epoch 2/2: 129it [00:20,  6.37it/s]
Epoch 2/2: 145it [00:23,  6.27it/s]
Epoch 2/2: 161it [00:26,  6.14it/s]
Epoch 2/2: 177it [00:29,  6.08it/s]
Epoch 2/2: 181it [00:29,  6.18it/s]
Epoch 2: Processed 88514 samples in 29.30s (3020.71 images/sec)

Benchmark Summary:
  Total samples: 177028
  Wall time: 62.34s
  Throughput: 3020.71 images/sec
  Time to first batch: 4.9926s
Benchmark complete
Cleaning up cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/webdataset_benchmark


========== MosaicML MDS ==========
Seed set to 42
Seed set to 42
Using remote path: s3://datology-assets-dev/users/haoli/shards/mds
Cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/mds_benchmark
Using cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/mds_benchmark
Using 16 worker threads for data loading
Starting benchmark with batch size 512

Epoch 1/2:   0%|          | 0/173 [00:00<?, ?it/s]
Epoch 1/2:   1%|          | 1/173 [00:06<19:25,  6.78s/it]
Epoch 1/2:  10%|▉         | 17/173 [00:09<01:23,  1.86it/s]
Epoch 1/2:  19%|█▉        | 33/173 [00:11<00:48,  2.89it/s]
Epoch 1/2:  28%|██▊       | 49/173 [00:13<00:33,  3.65it/s]
Epoch 1/2:  38%|███▊      | 65/173 [00:15<00:26,  4.12it/s]
Epoch 1/2:  47%|████▋     | 81/173 [00:18<00:20,  4.48it/s]
Epoch 1/2:  56%|█████▌    | 97/173 [00:20<00:15,  4.82it/s]
Epoch 1/2:  65%|██████▌   | 113/173 [00:22<00:11,  5.11it/s]
Epoch 1/2:  75%|███████▍  | 129/173 [00:24<00:08,  5.30it/s]
Epoch 1/2:  84%|████████▍ | 145/173 [00:26<00:05,  5.49it/s]
Epoch 1/2:  93%|█████████▎| 161/173 [00:28<00:02,  5.70it/s]
Epoch 1/2: 100%|██████████| 173/173 [00:29<00:00,  5.87it/s]
Time to first batch: 6.7799s
Epoch 1: Processed 88514 samples in 29.46s (3004.62 images/sec)

Epoch 2/2:   0%|          | 0/173 [00:00<?, ?it/s]
Epoch 2/2:   1%|          | 1/173 [00:02<08:16,  2.89s/it]
Epoch 2/2:  10%|▉         | 17/173 [00:05<00:46,  3.34it/s]
Epoch 2/2:  19%|█▉        | 33/173 [00:07<00:30,  4.59it/s]
Epoch 2/2:  28%|██▊       | 49/173 [00:09<00:23,  5.30it/s]
Epoch 2/2:  38%|███▊      | 65/173 [00:11<00:18,  5.78it/s]
Epoch 2/2:  47%|████▋     | 81/173 [00:13<00:15,  6.13it/s]
Epoch 2/2:  56%|█████▌    | 97/173 [00:15<00:11,  6.36it/s]
Epoch 2/2:  65%|██████▌   | 113/173 [00:17<00:09,  6.54it/s]
Epoch 2/2:  75%|███████▍  | 129/173 [00:19<00:06,  6.68it/s]
Epoch 2/2:  84%|████████▍ | 145/173 [00:21<00:04,  6.81it/s]
Epoch 2/2:  93%|█████████▎| 161/173 [00:22<00:01,  7.04it/s]
Epoch 2/2: 100%|██████████| 173/173 [00:23<00:00,  7.23it/s]
Epoch 2: Processed 88514 samples in 23.93s (3698.36 images/sec)

Benchmark Summary:
  Total samples: 177028
  Wall time: 53.39s
  Throughput: 3698.36 images/sec
  Time to first batch: 6.7799s
Benchmark complete
Cleaning up cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/mds_benchmark


========== LitData ==========
Seed set to 42
/home/ec2-user/benchmark-dataloader/stream/lightning_data.py:77: A newer version of litdata is available (0.2.45). Please consider upgrading with `pip install -U litdata`. Not all functionalities of the platform can be guaranteed to work with the current version.
Seed set to 42
Using S3 path from environment...
Using remote path: s3://datology-assets-dev/users/haoli/shards/litdata
Using remote path: s3://datology-assets-dev/users/haoli/shards/litdata
Cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/litdata_benchmark
Using cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/litdata_benchmark
Using 16 worker threads for data loading
Starting benchmark with batch size 512

Epoch 1/2:   0%|          | 0/173 [00:00<?, ?it/s]
Epoch 1/2:   1%|          | 1/173 [00:02<07:26,  2.59s/it]
Epoch 1/2:  10%|▉         | 17/173 [00:04<00:42,  3.65it/s]
Epoch 1/2:  19%|█▉        | 33/173 [00:06<00:28,  4.90it/s]
Epoch 1/2:  28%|██▊       | 49/173 [00:08<00:22,  5.50it/s]
Epoch 1/2:  38%|███▊      | 65/173 [00:11<00:18,  5.91it/s]
Epoch 1/2:  47%|████▋     | 81/173 [00:13<00:14,  6.14it/s]
Epoch 1/2:  56%|█████▌    | 97/173 [00:15<00:11,  6.37it/s]
Epoch 1/2:  65%|██████▌   | 113/173 [00:17<00:09,  6.46it/s]
Epoch 1/2:  75%|███████▍  | 129/173 [00:19<00:06,  6.57it/s]
Epoch 1/2:  84%|████████▍ | 145/173 [00:21<00:04,  6.69it/s]
Epoch 1/2:  93%|█████████▎| 161/173 [00:23<00:01,  6.86it/s]
Epoch 1/2: 100%|██████████| 173/173 [00:23<00:00,  7.27it/s]
Time to first batch: 2.6006s
Epoch 1: Processed 88514 samples in 23.80s (3718.90 images/sec)

Epoch 2/2:   0%|          | 0/173 [00:00<?, ?it/s]
Epoch 2/2:   1%|          | 1/173 [00:02<06:41,  2.34s/it]
Epoch 2/2:  10%|▉         | 17/173 [00:04<00:40,  3.83it/s]
Epoch 2/2:  19%|█▉        | 33/173 [00:06<00:27,  5.12it/s]
Epoch 2/2:  28%|██▊       | 49/173 [00:08<00:21,  5.71it/s]
Epoch 2/2:  38%|███▊      | 65/173 [00:10<00:17,  6.13it/s]
Epoch 2/2:  47%|████▋     | 81/173 [00:12<00:14,  6.41it/s]
Epoch 2/2:  56%|█████▌    | 97/173 [00:14<00:11,  6.59it/s]
Epoch 2/2:  65%|██████▌   | 113/173 [00:16<00:08,  6.71it/s]
Epoch 2/2:  75%|███████▍  | 129/173 [00:18<00:06,  6.83it/s]
Epoch 2/2:  84%|████████▍ | 145/173 [00:20<00:04,  6.93it/s]
Epoch 2/2:  93%|█████████▎| 161/173 [00:23<00:01,  6.99it/s]
Epoch 2/2: 100%|██████████| 173/173 [00:23<00:00,  7.41it/s]
Epoch 2: Processed 88514 samples in 23.35s (3791.25 images/sec)

Benchmark Summary:
  Total samples: 177028
  Wall time: 47.15s
  Throughput: 3791.25 images/sec
  Time to first batch: 2.6006s
Benchmark complete
Benchmark complete
Cleaning up cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/litdata_benchmark


========== Energon ==========
Seed set to 42
/home/ec2-user/benchmark-dataloader/.venv/lib/python3.13/site-packages/megatron/energon/loader.py:89: FutureWarning: Passing a worker_config to get_loader() is deprecated and will have no effect.
  warn_deprecated(
Using MSC_CONFIG: /home/ec2-user/benchmark-dataloader/stream/.msc_config.yaml
Using remote path: s3://datology-assets-dev/users/haoli/shards/energon
rank=0, worker=0: sample_range=[0, 5533] in 55 slices, sum(count)=5533: indexes=[0, 100, 201 ...<50> 5331, 5432, 5533] slices=[benchmark-train-000000.tar[0(start), 100], benchmark-train-000000.tar[100, 201], benchmark-train-000000.tar[201, 301] ...<50> benchmark-train-000001.tar[1410, 1511], benchmark-train-000001.tar[1511, 1612], benchmark-train-000001.tar[1612, 1713]]
rank=0, worker=1: sample_range=[5533, 11065] in 55 slices, sum(count)=5532: indexes=[5533, 5632, 5732 ...<50> 10862, 10963, 11065] slices=[benchmark-train-000001.tar[1713, 1812], benchmark-train-000001.tar[1812, 1912], benchmark-train-000001.tar[1912, 2012] ...<50> benchmark-train-000002.tar[3034, 3135], benchmark-train-000002.tar[3135, 3236], benchmark-train-000002.tar[3236, 3338]]
rank=0, worker=2: sample_range=[11065, 16597] in 55 slices, sum(count)=5532: indexes=[11065, 11174, 11283 ...<50> 16404, 16500, 16597] slices=[benchmark-train-000002.tar[3338, 3447], benchmark-train-000002.tar[3447, 3556], benchmark-train-000002.tar[3556, 3665] ...<50> benchmark-train-000004.tar[768, 864], benchmark-train-000004.tar[864, 960], benchmark-train-000004.tar[960, 1057]]
rank=0, worker=3: sample_range=[16597, 22129] in 56 slices, sum(count)=5532: indexes=[16597, 16695, 16793 ...<51> 21930, 22029, 22129] slices=[benchmark-train-000004.tar[1057, 1155], benchmark-train-000004.tar[1155, 1253], benchmark-train-000004.tar[1253, 1351] ...<51> benchmark-train-000005.tar[2483, 2583], benchmark-train-000005.tar[2583, 2682], benchmark-train-000005.tar[2682, 2782]]
rank=0, worker=4: sample_range=[22129, 27661] in 56 slices, sum(count)=5532: indexes=[22129, 22228, 22328 ...<51> 27468, 27564, 27661] slices=[benchmark-train-000005.tar[2782, 2881], benchmark-train-000005.tar[2881, 2981], benchmark-train-000005.tar[2981, 3080] ...<51> benchmark-train-000007.tar[385, 482], benchmark-train-000007.tar[482, 578], benchmark-train-000007.tar[578, 675]]
rank=0, worker=5: sample_range=[27661, 33193] in 56 slices, sum(count)=5532: indexes=[27661, 27760, 27859 ...<51> 32996, 33094, 33193] slices=[benchmark-train-000007.tar[675, 774], benchmark-train-000007.tar[774, 873], benchmark-train-000007.tar[873, 972] ...<51> benchmark-train-000008.tar[2067, 2166], benchmark-train-000008.tar[2166, 2264], benchmark-train-000008.tar[2264, 2363]]
rank=0, worker=6: sample_range=[33193, 38725] in 56 slices, sum(count)=5532: indexes=[33193, 33292, 33391 ...<51> 38556, 38640, 38725] slices=[benchmark-train-000008.tar[2363, 2462], benchmark-train-000008.tar[2462, 2561], benchmark-train-000008.tar[2561, 2660] ...<51> benchmark-train-000009.tar[3776, 3876(end)], benchmark-train-000010.tar[0(start), 84], benchmark-train-000010.tar[84, 169]]
rank=0, worker=7: sample_range=[38725, 44257] in 55 slices, sum(count)=5532: indexes=[38725, 38826, 38927 ...<50> 44057, 44157, 44257] slices=[benchmark-train-000010.tar[169, 270], benchmark-train-000010.tar[270, 371], benchmark-train-000010.tar[371, 472] ...<50> benchmark-train-000011.tar[1493, 1592], benchmark-train-000011.tar[1592, 1692], benchmark-train-000011.tar[1692, 1792]]
rank=0, worker=8: sample_range=[44257, 49790] in 55 slices, sum(count)=5533: indexes=[44257, 44356, 44456 ...<50> 49588, 49689, 49790] slices=[benchmark-train-000011.tar[1792, 1891], benchmark-train-000011.tar[1891, 1991], benchmark-train-000011.tar[1991, 2091] ...<50> benchmark-train-000012.tar[3231, 3332], benchmark-train-000012.tar[3332, 3433], benchmark-train-000012.tar[3433, 3534]]
rank=0, worker=9: sample_range=[49790, 55322] in 56 slices, sum(count)=5532: indexes=[49790, 49894, 49998 ...<51> 55128, 55225, 55322] slices=[benchmark-train-000012.tar[3534, 3638], benchmark-train-000012.tar[3638, 3742], benchmark-train-000012.tar[3742, 3846(end)] ...<51> benchmark-train-000014.tar[1066, 1163], benchmark-train-000014.tar[1163, 1260], benchmark-train-000014.tar[1260, 1357]]
rank=0, worker=10: sample_range=[55322, 60854] in 55 slices, sum(count)=5532: indexes=[55322, 55421, 55521 ...<50> 60651, 60752, 60854] slices=[benchmark-train-000014.tar[1357, 1456], benchmark-train-000014.tar[1456, 1556], benchmark-train-000014.tar[1556, 1655] ...<50> benchmark-train-000015.tar[2736, 2838], benchmark-train-000015.tar[2838, 2939], benchmark-train-000015.tar[2939, 3041]]
rank=0, worker=11: sample_range=[60854, 66386] in 55 slices, sum(count)=5532: indexes=[60854, 60956, 61059 ...<50> 66180, 66283, 66386] slices=[benchmark-train-000015.tar[3041, 3143], benchmark-train-000015.tar[3143, 3246], benchmark-train-000015.tar[3246, 3349] ...<50> benchmark-train-000017.tar[410, 512], benchmark-train-000017.tar[512, 615], benchmark-train-000017.tar[615, 718]]
rank=0, worker=12: sample_range=[66386, 71918] in 55 slices, sum(count)=5532: indexes=[66386, 66487, 66588 ...<50> 71718, 71818, 71918] slices=[benchmark-train-000017.tar[718, 819], benchmark-train-000017.tar[819, 920], benchmark-train-000017.tar[920, 1021] ...<50> benchmark-train-000018.tar[1995, 2095], benchmark-train-000018.tar[2095, 2195], benchmark-train-000018.tar[2195, 2295]]
rank=0, worker=13: sample_range=[71918, 77450] in 55 slices, sum(count)=5532: indexes=[71918, 72018, 72118 ...<50> 77254, 77355, 77450] slices=[benchmark-train-000018.tar[2295, 2395], benchmark-train-000018.tar[2395, 2495], benchmark-train-000018.tar[2495, 2596] ...<50> benchmark-train-000019.tar[3628, 3729], benchmark-train-000019.tar[3729, 3830(end)], benchmark-train-000020.tar[0(start), 95]]
rank=0, worker=14: sample_range=[77450, 82982] in 56 slices, sum(count)=5532: indexes=[77450, 77549, 77648 ...<51> 82785, 82883, 82982] slices=[benchmark-train-000020.tar[95, 194], benchmark-train-000020.tar[194, 293], benchmark-train-000020.tar[293, 392] ...<51> benchmark-train-000021.tar[1471, 1569], benchmark-train-000021.tar[1569, 1667], benchmark-train-000021.tar[1667, 1766]]
rank=0, worker=15: sample_range=[82982, 88514] in 55 slices, sum(count)=5532: indexes=[82982, 83082, 83183 ...<50> 88313, 88413, 88514] slices=[benchmark-train-000021.tar[1766, 1866], benchmark-train-000021.tar[1866, 1967], benchmark-train-000021.tar[1967, 2068] ...<50> benchmark-train-000022.tar[3114, 3215], benchmark-train-000022.tar[3215, 3315], benchmark-train-000022.tar[3315, 3416(end)]]
Starting benchmark with batch size 512

Epoch 1/2:   0%|          | 0/173 [00:00<?, ?it/s]
Epoch 1/2:   1%|          | 1/173 [00:10<29:10, 10.18s/it]
Epoch 1/2:   3%|▎         | 5/173 [00:13<07:43,  2.76s/it]
Epoch 1/2:  12%|█▏        | 21/173 [00:16<02:00,  1.26it/s]
Epoch 1/2:  21%|██▏       | 37/173 [00:20<01:15,  1.81it/s]
Epoch 1/2:  31%|███       | 53/173 [00:25<00:57,  2.10it/s]
Epoch 1/2:  40%|███▉      | 69/173 [00:28<00:42,  2.46it/s]
Epoch 1/2:  49%|████▉     | 85/173 [00:31<00:32,  2.68it/s]
Epoch 1/2:  58%|█████▊    | 101/173 [00:36<00:26,  2.75it/s]
Epoch 1/2:  68%|██████▊   | 117/173 [00:39<00:18,  2.95it/s]
Epoch 1/2:  77%|███████▋  | 133/173 [00:43<00:13,  3.07it/s]
Epoch 1/2:  86%|████████▌ | 149/173 [00:46<00:07,  3.22it/s]
Epoch 1/2:  95%|█████████▌| 165/173 [00:48<00:02,  3.39it/s]
Epoch 1/2: 100%|██████████| 173/173 [00:48<00:00,  3.55it/s]
Time to first batch: 10.1810s
Epoch 1: Processed 88576 samples in 48.68s (1819.63 images/sec)

Epoch 2/2:   0%|          | 0/173 [00:00<?, ?it/s]
Epoch 2/2:   1%|          | 1/173 [00:03<11:03,  3.86s/it]
Epoch 2/2:  10%|▉         | 17/173 [00:07<01:07,  2.33it/s]
Epoch 2/2:  19%|█▉        | 33/173 [00:11<00:50,  2.79it/s]
Epoch 2/2:  28%|██▊       | 49/173 [00:15<00:39,  3.12it/s]
Epoch 2/2:  38%|███▊      | 65/173 [00:18<00:31,  3.48it/s]
Epoch 2/2:  47%|████▋     | 81/173 [00:23<00:26,  3.52it/s]
Epoch 2/2:  56%|█████▌    | 97/173 [00:27<00:21,  3.59it/s]
Epoch 2/2:  65%|██████▌   | 113/173 [00:30<00:16,  3.70it/s]
Epoch 2/2:  75%|███████▍  | 129/173 [00:34<00:11,  3.74it/s]
Epoch 2/2:  84%|████████▍ | 145/173 [00:38<00:07,  3.81it/s]
Epoch 2/2:  93%|█████████▎| 161/173 [00:41<00:03,  3.90it/s]
Epoch 2/2: 100%|██████████| 173/173 [00:41<00:00,  4.18it/s]
Epoch 2: Processed 88576 samples in 41.42s (2138.65 images/sec)

Benchmark Summary:
  Total samples: 177152
  Wall time: 90.09s
  Throughput: 2138.65 images/sec
  Time to first batch: 10.1810s
Benchmark complete
