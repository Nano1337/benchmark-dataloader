DATASET STREAMING BENCHMARKS
Started at: 2025-04-25 08:02:27
Configuration: batch_size=1024, num_workers=16, prefetch_factor=2



========== WebDataset ==========
Seed set to 42
/home/ec2-user/benchmark-dataloader/.venv/lib/python3.13/site-packages/webdataset/compat.py:389: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number
  warnings.warn(
Seed set to 42
Using remote path: s3://datology-assets-dev/users/haoli/shards/webdataset
Using remote path: s3://datology-assets-dev/users/haoli/shards/webdataset
Listing objects...
Found 23 objects in S3
Found 23 train tar files matching pattern 'benchmark-train-'
Cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/webdataset_benchmark
Using cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/webdataset_benchmark
Using 16 worker threads for data loading
Starting benchmark with batch size 1024

Epoch 1/2: 0it [00:00, ?it/s]
Epoch 1/2: 1it [00:07,  7.44s/it]
Epoch 1/2: 17it [00:12,  1.36it/s]
Epoch 1/2: 33it [00:16,  1.95it/s]
Epoch 1/2: 49it [00:22,  2.15it/s]
Epoch 1/2: 65it [00:25,  2.53it/s]
Epoch 1/2: 81it [00:31,  2.58it/s]
Epoch 1/2: 92it [00:33,  2.77it/s]
Time to first batch: 7.4427s
Epoch 1: Processed 88514 samples in 33.22s (2664.78 images/sec)

Epoch 2/2: 0it [00:00, ?it/s]
Epoch 2/2: 1it [00:05,  5.45s/it]
Epoch 2/2: 17it [00:10,  1.65it/s]
Epoch 2/2: 33it [00:15,  2.18it/s]
Epoch 2/2: 49it [00:19,  2.56it/s]
Epoch 2/2: 65it [00:21,  2.96it/s]
Epoch 2/2: 81it [00:27,  2.95it/s]
Epoch 2/2: 92it [00:29,  3.14it/s]
Epoch 2: Processed 88514 samples in 29.34s (3017.09 images/sec)

Benchmark Summary:
  Total samples: 177028
  Wall time: 62.55s
  Throughput: 3017.09 images/sec
  Time to first batch: 7.4427s
Benchmark complete
Cleaning up cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/webdataset_benchmark


========== MosaicML MDS ==========
Seed set to 42
Seed set to 42
Using remote path: s3://datology-assets-dev/users/haoli/shards/mds
Cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/mds_benchmark
Using cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/mds_benchmark
Using 16 worker threads for data loading
Starting benchmark with batch size 1024

Epoch 1/2:   0%|          | 0/87 [00:00<?, ?it/s]
Epoch 1/2:   1%|          | 1/87 [00:09<13:53,  9.69s/it]
Epoch 1/2:  20%|█▉        | 17/87 [00:14<00:57,  1.21it/s]
Epoch 1/2:  38%|███▊      | 33/87 [00:18<00:29,  1.82it/s]
Epoch 1/2:  56%|█████▋    | 49/87 [00:22<00:17,  2.18it/s]
Epoch 1/2:  75%|███████▍  | 65/87 [00:26<00:08,  2.46it/s]
Epoch 1/2:  93%|█████████▎| 81/87 [00:28<00:02,  2.80it/s]
Epoch 1/2: 100%|██████████| 87/87 [00:29<00:00,  2.99it/s]
Time to first batch: 9.6937s
Epoch 1: Processed 88514 samples in 29.12s (3039.14 images/sec)

Epoch 2/2:   0%|          | 0/87 [00:00<?, ?it/s]
Epoch 2/2:   1%|          | 1/87 [00:05<07:11,  5.02s/it]
Epoch 2/2:  20%|█▉        | 17/87 [00:09<00:37,  1.84it/s]
Epoch 2/2:  38%|███▊      | 33/87 [00:13<00:21,  2.48it/s]
Epoch 2/2:  56%|█████▋    | 49/87 [00:17<00:13,  2.81it/s]
Epoch 2/2:  75%|███████▍  | 65/87 [00:21<00:07,  3.03it/s]
Epoch 2/2:  93%|█████████▎| 81/87 [00:24<00:01,  3.37it/s]
Epoch 2/2: 100%|██████████| 87/87 [00:24<00:00,  3.57it/s]
Epoch 2: Processed 88514 samples in 24.38s (3630.61 images/sec)

Benchmark Summary:
  Total samples: 177028
  Wall time: 53.50s
  Throughput: 3630.61 images/sec
  Time to first batch: 9.6937s
Benchmark complete
Cleaning up cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/mds_benchmark


========== LitData ==========
Seed set to 42
/home/ec2-user/benchmark-dataloader/stream/lightning_data.py:77: A newer version of litdata is available (0.2.45). Please consider upgrading with `pip install -U litdata`. Not all functionalities of the platform can be guaranteed to work with the current version.
Seed set to 42
Using S3 path from environment...
Using remote path: s3://datology-assets-dev/users/haoli/shards/litdata
Using remote path: s3://datology-assets-dev/users/haoli/shards/litdata
Cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/litdata_benchmark
Using cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/litdata_benchmark
Using 16 worker threads for data loading
Starting benchmark with batch size 1024

Epoch 1/2:   0%|          | 0/87 [00:00<?, ?it/s]
Epoch 1/2:   1%|          | 1/87 [00:04<06:42,  4.69s/it]
Epoch 1/2:  20%|█▉        | 17/87 [00:08<00:36,  1.93it/s]
Epoch 1/2:  38%|███▊      | 33/87 [00:12<00:21,  2.54it/s]
Epoch 1/2:  56%|█████▋    | 49/87 [00:17<00:13,  2.84it/s]
Epoch 1/2:  75%|███████▍  | 65/87 [00:21<00:07,  3.07it/s]
Epoch 1/2:  93%|█████████▎| 81/87 [00:23<00:01,  3.39it/s]
Epoch 1/2: 100%|██████████| 87/87 [00:24<00:00,  3.61it/s]
Time to first batch: 4.6910s
Epoch 1: Processed 88514 samples in 24.10s (3673.28 images/sec)

Epoch 2/2:   0%|          | 0/87 [00:00<?, ?it/s]
Epoch 2/2:   1%|          | 1/87 [00:04<06:21,  4.44s/it]
Epoch 2/2:  20%|█▉        | 17/87 [00:08<00:35,  1.98it/s]
Epoch 2/2:  38%|███▊      | 33/87 [00:12<00:20,  2.59it/s]
Epoch 2/2:  56%|█████▋    | 49/87 [00:16<00:13,  2.89it/s]
Epoch 2/2:  75%|███████▍  | 65/87 [00:20<00:07,  3.11it/s]
Epoch 2/2:  93%|█████████▎| 81/87 [00:23<00:01,  3.42it/s]
Epoch 2/2: 100%|██████████| 87/87 [00:24<00:00,  3.61it/s]
Epoch 2: Processed 88514 samples in 24.13s (3667.91 images/sec)

Benchmark Summary:
  Total samples: 177028
  Wall time: 48.23s
  Throughput: 3667.91 images/sec
  Time to first batch: 4.6910s
Benchmark complete
Benchmark complete
Cleaning up cache directory: /home/ec2-user/benchmark-dataloader/stream/cache/litdata_benchmark


========== Energon ==========
Seed set to 42
/home/ec2-user/benchmark-dataloader/.venv/lib/python3.13/site-packages/megatron/energon/loader.py:89: FutureWarning: Passing a worker_config to get_loader() is deprecated and will have no effect.
  warn_deprecated(
Using MSC_CONFIG: /home/ec2-user/benchmark-dataloader/stream/.msc_config.yaml
Using remote path: s3://datology-assets-dev/users/haoli/shards/energon
rank=0, worker=0: sample_range=[0, 5533] in 55 slices, sum(count)=5533: indexes=[0, 100, 201 ...<50> 5331, 5432, 5533] slices=[benchmark-train-000000.tar[0(start), 100], benchmark-train-000000.tar[100, 201], benchmark-train-000000.tar[201, 301] ...<50> benchmark-train-000001.tar[1410, 1511], benchmark-train-000001.tar[1511, 1612], benchmark-train-000001.tar[1612, 1713]]
rank=0, worker=1: sample_range=[5533, 11065] in 55 slices, sum(count)=5532: indexes=[5533, 5632, 5732 ...<50> 10862, 10963, 11065] slices=[benchmark-train-000001.tar[1713, 1812], benchmark-train-000001.tar[1812, 1912], benchmark-train-000001.tar[1912, 2012] ...<50> benchmark-train-000002.tar[3034, 3135], benchmark-train-000002.tar[3135, 3236], benchmark-train-000002.tar[3236, 3338]]
rank=0, worker=2: sample_range=[11065, 16597] in 55 slices, sum(count)=5532: indexes=[11065, 11174, 11283 ...<50> 16404, 16500, 16597] slices=[benchmark-train-000002.tar[3338, 3447], benchmark-train-000002.tar[3447, 3556], benchmark-train-000002.tar[3556, 3665] ...<50> benchmark-train-000004.tar[768, 864], benchmark-train-000004.tar[864, 960], benchmark-train-000004.tar[960, 1057]]
rank=0, worker=3: sample_range=[16597, 22129] in 56 slices, sum(count)=5532: indexes=[16597, 16695, 16793 ...<51> 21930, 22029, 22129] slices=[benchmark-train-000004.tar[1057, 1155], benchmark-train-000004.tar[1155, 1253], benchmark-train-000004.tar[1253, 1351] ...<51> benchmark-train-000005.tar[2483, 2583], benchmark-train-000005.tar[2583, 2682], benchmark-train-000005.tar[2682, 2782]]
rank=0, worker=4: sample_range=[22129, 27661] in 56 slices, sum(count)=5532: indexes=[22129, 22228, 22328 ...<51> 27468, 27564, 27661] slices=[benchmark-train-000005.tar[2782, 2881], benchmark-train-000005.tar[2881, 2981], benchmark-train-000005.tar[2981, 3080] ...<51> benchmark-train-000007.tar[385, 482], benchmark-train-000007.tar[482, 578], benchmark-train-000007.tar[578, 675]]
rank=0, worker=5: sample_range=[27661, 33193] in 56 slices, sum(count)=5532: indexes=[27661, 27760, 27859 ...<51> 32996, 33094, 33193] slices=[benchmark-train-000007.tar[675, 774], benchmark-train-000007.tar[774, 873], benchmark-train-000007.tar[873, 972] ...<51> benchmark-train-000008.tar[2067, 2166], benchmark-train-000008.tar[2166, 2264], benchmark-train-000008.tar[2264, 2363]]
rank=0, worker=6: sample_range=[33193, 38725] in 56 slices, sum(count)=5532: indexes=[33193, 33292, 33391 ...<51> 38556, 38640, 38725] slices=[benchmark-train-000008.tar[2363, 2462], benchmark-train-000008.tar[2462, 2561], benchmark-train-000008.tar[2561, 2660] ...<51> benchmark-train-000009.tar[3776, 3876(end)], benchmark-train-000010.tar[0(start), 84], benchmark-train-000010.tar[84, 169]]
rank=0, worker=7: sample_range=[38725, 44257] in 55 slices, sum(count)=5532: indexes=[38725, 38826, 38927 ...<50> 44057, 44157, 44257] slices=[benchmark-train-000010.tar[169, 270], benchmark-train-000010.tar[270, 371], benchmark-train-000010.tar[371, 472] ...<50> benchmark-train-000011.tar[1493, 1592], benchmark-train-000011.tar[1592, 1692], benchmark-train-000011.tar[1692, 1792]]
rank=0, worker=8: sample_range=[44257, 49790] in 55 slices, sum(count)=5533: indexes=[44257, 44356, 44456 ...<50> 49588, 49689, 49790] slices=[benchmark-train-000011.tar[1792, 1891], benchmark-train-000011.tar[1891, 1991], benchmark-train-000011.tar[1991, 2091] ...<50> benchmark-train-000012.tar[3231, 3332], benchmark-train-000012.tar[3332, 3433], benchmark-train-000012.tar[3433, 3534]]
rank=0, worker=9: sample_range=[49790, 55322] in 56 slices, sum(count)=5532: indexes=[49790, 49894, 49998 ...<51> 55128, 55225, 55322] slices=[benchmark-train-000012.tar[3534, 3638], benchmark-train-000012.tar[3638, 3742], benchmark-train-000012.tar[3742, 3846(end)] ...<51> benchmark-train-000014.tar[1066, 1163], benchmark-train-000014.tar[1163, 1260], benchmark-train-000014.tar[1260, 1357]]
rank=0, worker=10: sample_range=[55322, 60854] in 55 slices, sum(count)=5532: indexes=[55322, 55421, 55521 ...<50> 60651, 60752, 60854] slices=[benchmark-train-000014.tar[1357, 1456], benchmark-train-000014.tar[1456, 1556], benchmark-train-000014.tar[1556, 1655] ...<50> benchmark-train-000015.tar[2736, 2838], benchmark-train-000015.tar[2838, 2939], benchmark-train-000015.tar[2939, 3041]]
rank=0, worker=11: sample_range=[60854, 66386] in 55 slices, sum(count)=5532: indexes=[60854, 60956, 61059 ...<50> 66180, 66283, 66386] slices=[benchmark-train-000015.tar[3041, 3143], benchmark-train-000015.tar[3143, 3246], benchmark-train-000015.tar[3246, 3349] ...<50> benchmark-train-000017.tar[410, 512], benchmark-train-000017.tar[512, 615], benchmark-train-000017.tar[615, 718]]
rank=0, worker=12: sample_range=[66386, 71918] in 55 slices, sum(count)=5532: indexes=[66386, 66487, 66588 ...<50> 71718, 71818, 71918] slices=[benchmark-train-000017.tar[718, 819], benchmark-train-000017.tar[819, 920], benchmark-train-000017.tar[920, 1021] ...<50> benchmark-train-000018.tar[1995, 2095], benchmark-train-000018.tar[2095, 2195], benchmark-train-000018.tar[2195, 2295]]
rank=0, worker=13: sample_range=[71918, 77450] in 55 slices, sum(count)=5532: indexes=[71918, 72018, 72118 ...<50> 77254, 77355, 77450] slices=[benchmark-train-000018.tar[2295, 2395], benchmark-train-000018.tar[2395, 2495], benchmark-train-000018.tar[2495, 2596] ...<50> benchmark-train-000019.tar[3628, 3729], benchmark-train-000019.tar[3729, 3830(end)], benchmark-train-000020.tar[0(start), 95]]
rank=0, worker=14: sample_range=[77450, 82982] in 56 slices, sum(count)=5532: indexes=[77450, 77549, 77648 ...<51> 82785, 82883, 82982] slices=[benchmark-train-000020.tar[95, 194], benchmark-train-000020.tar[194, 293], benchmark-train-000020.tar[293, 392] ...<51> benchmark-train-000021.tar[1471, 1569], benchmark-train-000021.tar[1569, 1667], benchmark-train-000021.tar[1667, 1766]]
rank=0, worker=15: sample_range=[82982, 88514] in 55 slices, sum(count)=5532: indexes=[82982, 83082, 83183 ...<50> 88313, 88413, 88514] slices=[benchmark-train-000021.tar[1766, 1866], benchmark-train-000021.tar[1866, 1967], benchmark-train-000021.tar[1967, 2068] ...<50> benchmark-train-000022.tar[3114, 3215], benchmark-train-000022.tar[3215, 3315], benchmark-train-000022.tar[3315, 3416(end)]]
Starting benchmark with batch size 1024

Epoch 1/2:   0%|          | 0/87 [00:00<?, ?it/s]
Epoch 1/2:   1%|          | 1/87 [00:13<19:17, 13.45s/it]
Epoch 1/2:   3%|▎         | 3/87 [00:14<06:49,  4.87s/it]
Epoch 1/2:   6%|▌         | 5/87 [00:16<04:32,  3.33s/it]
Epoch 1/2:  20%|█▉        | 17/87 [00:21<01:27,  1.26s/it]
Epoch 1/2:  33%|███▎      | 29/87 [00:25<00:50,  1.16it/s]
Epoch 1/2:  47%|████▋     | 41/87 [00:31<00:35,  1.29it/s]
Epoch 1/2:  61%|██████    | 53/87 [00:39<00:25,  1.34it/s]
Epoch 1/2:  75%|███████▍  | 65/87 [00:42<00:14,  1.51it/s]
Epoch 1/2:  89%|████████▊ | 77/87 [00:46<00:05,  1.67it/s]
Epoch 1/2: 100%|██████████| 87/87 [00:50<00:00,  1.72it/s]
Time to first batch: 13.4554s
Epoch 1: Processed 89088 samples in 50.66s (1758.52 images/sec)

Epoch 2/2:   0%|          | 0/87 [00:00<?, ?it/s]
Epoch 2/2:   1%|          | 1/87 [00:08<11:49,  8.26s/it]
Epoch 2/2:  20%|█▉        | 17/87 [00:14<01:01,  1.14it/s]
Epoch 2/2:  38%|███▊      | 33/87 [00:23<00:38,  1.41it/s]
Epoch 2/2:  56%|█████▋    | 49/87 [00:30<00:23,  1.62it/s]
Epoch 2/2:  75%|███████▍  | 65/87 [00:37<00:12,  1.72it/s]
Epoch 2/2:  93%|█████████▎| 81/87 [00:42<00:03,  1.91it/s]
Epoch 2/2: 100%|██████████| 87/87 [00:42<00:00,  2.03it/s]
Epoch 2: Processed 89088 samples in 42.83s (2079.94 images/sec)

Benchmark Summary:
  Total samples: 178176
  Wall time: 93.49s
  Throughput: 2079.94 images/sec
  Time to first batch: 13.4554s
Benchmark complete
